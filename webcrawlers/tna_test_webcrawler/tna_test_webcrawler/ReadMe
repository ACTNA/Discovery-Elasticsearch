TNA Web crawlers:

This document describes the implementation of a basic TNA Webcrawler


spiders:
Spiders are classes which define how a certain site (or a group of sites) will be scraped, including how to perform
the crawl (i.e. follow links) and how to extract structured data from their pages (i.e. scraping items). In other
words, Spiders are the place where you define the custom behaviour for crawling and parsing pages for a particular
site (or, in some cases, a group of sites).

Items:
The main goal in scraping is to extract structured data from unstructured sources, typically, web pages. Scrapy
spiders can return the extracted data as Python dicts. While convenient and familiar, Python dicts lack structure:
it is easy to make a typo in a field name or return inconsistent data, especially in a larger project with many
spiders.

To define common output data format Scrapy provides the Item class. Item objects are simple containers used to collect
the scraped data. They provide a dictionary-like API with a convenient syntax for declaring their available fields.

Pipelines:
After an item has been scraped by a spider, it is sent to the Item Pipeline which processes it through several
components that are executed sequentially.

Each item pipeline component (sometimes referred as just “Item Pipeline”) is a Python class that implements a simple
method. They receive an item and perform an action over it, also deciding if the item should continue through the
pipeline or be dropped and no longer processed.

Settings:
The Scrapy settings allows you to customize the behaviour of all Scrapy components, including the core, extensions,
pipelines and spiders themselves.

The infrastructure of the settings provides a global namespace of key-value mappings that the code can use to pull
configuration values from. The settings can be populated through different mechanisms

Middlewares:
The spider middleware is a framework of hooks into Scrapy’s spider processing mechanism where you can plug custom
functionality to process the responses that are sent to Spiders for processing and to process the requests and items
that are generated from spiders.

Helpers:
Customs functions to help with parsing data
